{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\songz\\.conda\\envs\\demo\\Lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 0.24.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\songz\\.conda\\envs\\demo\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xff in position 21: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./audio/第01回 老教头失手埋祸 破落户帮闲遭逐.mp3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# y, sr = librosa.load(audio_path)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 这里使用pyAudioAnalysis进行说话人分割（示例，实际代码可能有所不同）\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m segments \u001b[38;5;241m=\u001b[39m \u001b[43maS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspeaker_diarization\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_speakers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# segments将包含每个音频帧的说话人标签\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 根据标签更换来确定说话人B的起始和终止位置\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\songz\\.conda\\envs\\demo\\Lib\\site-packages\\pyAudioAnalysis\\audioSegmentation.py:1016\u001b[0m, in \u001b[0;36mspeaker_diarization\u001b[1;34m(filename, n_speakers, mid_window, mid_step, short_window, lda_dim, plot_res)\u001b[0m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;66;03m# if groundtruth exists\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(gt_file):\n\u001b[1;32m-> 1016\u001b[0m     seg_start, seg_end, seg_labs \u001b[38;5;241m=\u001b[39m \u001b[43mread_segmentation_gt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1017\u001b[0m     flags_gt, class_names_gt \u001b[38;5;241m=\u001b[39m segments_to_labels(seg_start, seg_end,\n\u001b[0;32m   1018\u001b[0m                                                   seg_labs, mid_step)\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m plot_res:\n",
      "File \u001b[1;32mc:\\Users\\songz\\.conda\\envs\\demo\\Lib\\site-packages\\pyAudioAnalysis\\audioSegmentation.py:168\u001b[0m, in \u001b[0;36mread_segmentation_gt\u001b[1;34m(gt_file)\u001b[0m\n\u001b[0;32m    166\u001b[0m end_times \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    167\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 168\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_times\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xff in position 21: invalid start byte"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "# 假设`pyAudioAnalysis`库提供了说话人识别功能\n",
    "from pyAudioAnalysis import audioSegmentation as aS\n",
    "\n",
    "# 加载音频文件\n",
    "# audio_path = './audio/第01回 老教头失手埋祸 破落户帮闲遭逐.mp3'\n",
    "audio_path = './audio/第01回 老教头失手埋祸 破落户帮闲遭逐.mp3'\n",
    "# y, sr = librosa.load(audio_path)\n",
    "\n",
    "# 这里使用pyAudioAnalysis进行说话人分割（示例，实际代码可能有所不同）\n",
    "segments = aS.speaker_diarization(audio_path, n_speakers=2)\n",
    "\n",
    "# segments将包含每个音频帧的说话人标签\n",
    "# 根据标签更换来确定说话人B的起始和终止位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='./audio/001.flac'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "\n",
    "in_mp3_path = './audio/第01回 老教头失手埋祸 破落户帮闲遭逐.mp3'\n",
    "wav_path = './audio/001.wav'\n",
    "flac_path = './audio/001.flac'\n",
    "out_mp3_path = './audio/001.mp3'\n",
    "\n",
    "# 加载MP3文件\n",
    "audio = AudioSegment.from_mp3(in_mp3_path)\n",
    "\n",
    "# 导出为WAV\n",
    "# audio.export(wav_path, format=\"wav\")\n",
    "\n",
    "# 导出为FLAC\n",
    "audio.export(flac_path, format=\"flac\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分析音频以识别说话者\n",
    "使用pyAudioAnalysis进行说话者分离涉及到音频分割和特征提取，然后应用聚类算法来估计不同说话者的数量。以下是一个基本的示例代码，展示如何进行这些步骤：\n",
    "这段代码中的speaker_diarization函数会尝试对音频进行分割，并估计出不同的说话者。n_speakers=-1意味着函数将自动尝试确定说话者的数量。其他参数如mid_window、mid_step和short_window可以根据音频的具体特性进行调整以优化结果。\n",
    "\n",
    "请注意，实际的音频分析和说话者分离效果极大依赖于音频质量、说话者之间的交谈方式以及背景噪音等因素。因此，可能需要对参数进行一些调整才能获得最佳结果。\n",
    "\n",
    "pyAudioAnalysis的使用相对复杂，涉及许多参数和音频处理的细节，建议详细阅读其官方文档和教程，以便更好地理解如何使用这个库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\songz\\.conda\\envs\\demo\\Lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 0.24.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\songz\\.conda\\envs\\demo\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\songz\\.conda\\envs\\demo\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\songz\\.conda\\envs\\demo\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\songz\\.conda\\envs\\demo\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\songz\\.conda\\envs\\demo\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\songz\\.conda\\envs\\demo\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\songz\\.conda\\envs\\demo\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\songz\\.conda\\envs\\demo\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of speakers: 9\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "from pyAudioAnalysis import audioSegmentation as aS\n",
    "\n",
    "\n",
    "in_mp3_path = './audio/第01回 老教头失手埋祸 破落户帮闲遭逐.mp3'\n",
    "wav_path = './audio/001.wav'\n",
    "out_mp3_path = './audio/001.mp3'\n",
    "\n",
    "# 加载MP3文件\n",
    "audio = AudioSegment.from_mp3(in_mp3_path)\n",
    "\n",
    "# 导出为WAV\n",
    "audio.export(wav_path, format=\"wav\")\n",
    "\n",
    "# 使用说话者分割功能\n",
    "# 这里的参数可能需要根据你的音频进行调整\n",
    "segments, sr, _ = aS.speaker_diarization(filename=wav_path, n_speakers=2, mid_window=1.0, mid_step=0.1, short_window=0.05, lda_dim=35, plot_res=False)\n",
    "\n",
    "\n",
    "# 计算说话者数量\n",
    "num_speakers = len(set(segments))\n",
    "\n",
    "print(f\"Estimated number of speakers: {num_speakers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 语音识别\n",
    "`speech_recognition`是一个Python库，提供了对多个语音识别服务的接口，包括Google Web Speech API、Microsoft Bing Voice Recognition、IBM Speech to Text等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要安装speech_recognition库\n",
    "# pip install SpeechRecognition\n",
    "import speech_recognition as sr\n",
    "\n",
    "wav_path = './audio/001.wav'\n",
    "\n",
    "# 初始化识别器\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# 从音频文件中加载音频数据\n",
    "with sr.AudioFile(wav_path) as source:\n",
    "    audio_data = r.record(source)\n",
    "\n",
    "# 使用Google Web Speech API进行识别\n",
    "text = r.recognize_google(audio_data, language=\"zh-CN\")\n",
    "\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
